{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "dataset = np.genfromtxt(\"../train.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = np.genfromtxt(\"../test0.csv\",delimiter=',')\n",
    "test_eval = np.genfromtxt(\"../test0_eval.csv\",delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50282, 438)\n",
      "(1781, 438)\n"
     ]
    }
   ],
   "source": [
    "print dataset.shape\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "display_step = 10\n",
    "training_iters = 40000\n",
    "sess.run(init)\n",
    "step = 1\n",
    "batch_size = 50\n",
    "sampling_ratio = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 429 # data input (img shape: 13*33)\n",
    "n_classes = 98 # total classes\n",
    "dropout = 0.8 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x,ks=3,k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, ks, ks, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 33, 13, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, ks=3, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, ks=3, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv3 = maxpool2d(conv3, ks=3, k=2)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    fc2 = tf.add(tf.matmul(fc1, weights['wd2']), biases['bd2'])\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc2, weights['out']), biases['out'])\n",
    "    out = tf.nn.dropout(out, dropout)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 3x3 conv, 1 input, 64 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([3, 3, 1, 64])),\n",
    "    # 3x3 conv, 64 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([3, 3, 64, 64])),\n",
    "    # 3x3 conv, 64 inputs, 128 outputs\n",
    "    'wc3': tf.Variable(tf.random_normal([3, 3, 64, 128])),\n",
    "    # fully connected,  inputs 1280, 3072 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([5*2*128, 3072])),\n",
    "    # fully connected, inputs 3072, 2048 outputs\n",
    "    'wd2': tf.Variable(tf.random_normal([3072, 2048])),\n",
    "    # 2048 inputs, 98 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([2048, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([64])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bc3': tf.Variable(tf.random_normal([128])),\n",
    "    'bd1': tf.Variable(tf.random_normal([3072])),\n",
    "    'bd2': tf.Variable(tf.random_normal([2048])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def k_fold_cross_validation(data, K, randomise = True):\n",
    "#     d = copy.deepcopy(data)\n",
    "#     np.random.shuffle(d)\n",
    "#     size = d.shape[0]\n",
    "#     hold_out_size = int(size/K)\n",
    "#     for k in range(K):\n",
    "#         validation = d[(hold_out_size*k):(hold_out_size*(k+1)),:]\n",
    "#         if(k == 0):\n",
    "#             training = d[(hold_out_size*(k+1)):,:]\n",
    "#         else:\n",
    "#             training = np.append(d[(hold_out_size*(k+1)):,:],d[:(hold_out_size*k),:],axis=0)\n",
    "#         yield training,validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_batch(training, batch_size,iteration = -1):\n",
    "    train_example, train_label = data_process(training)\n",
    "    total_batch = int(training.shape[0]/batch_size)\n",
    "    if(iteration > 0 & iteration < total_batch):\n",
    "        total_batch = iteration\n",
    "    for i in range(total_batch):\n",
    "        low = i*batch_size\n",
    "        high = (i+1)*batch_size\n",
    "        yield train_example[low:high,:],train_label[low:high,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_process(d, input_size = 429, class_size = 98):\n",
    "    size = d.shape[0]\n",
    "    example = d[:,9:]\n",
    "    label = d[:, 1:2]\n",
    "    label_matrix = np.zeros((size,class_size))\n",
    "    for i in range(size):\n",
    "        label_matrix[i][int(label[i])] = 1\n",
    "    return example, label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_random_batch(data, batch_size, Split=True):\n",
    "    d = copy.deepcopy(data)\n",
    "    np.random.shuffle(d)\n",
    "    low = 0\n",
    "    high = batch_size\n",
    "    if(Split == False):\n",
    "        return data[low:high,:]\n",
    "    example, label = data_process(d)\n",
    "    return example[low:high,:],label[low:high,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_example, t_label = data_process(test_eval)\n",
    "result_note = np.zeros([0,98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    training = next_random_batch(dataset,int(dataset.shape[0]*sampling_ratio),Split=False)\n",
    "    flag = False\n",
    "    \n",
    "    while step * batch_size < training_iters:\n",
    "        test_acc = 0\n",
    "        for  _batch_xs, _batch_ys in next_batch(training,batch_size):\n",
    "        # Fit training using batch data\n",
    "            sess.run(optimizer, feed_dict={x: _batch_xs, y: _batch_ys, keep_prob: dropout})\n",
    "            acc = 0\n",
    "            if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "                acc = sess.run(accuracy, feed_dict={x: _batch_xs, y: _batch_ys, keep_prob: 1.})\n",
    "            # Calculate batch loss\n",
    "                loss = sess.run(cost, feed_dict={x: _batch_xs, y: _batch_ys, keep_prob: 1.})\n",
    "                test_acc = sess.run(accuracy, feed_dict={x: t_example, y: t_label, keep_prob: 1.})\n",
    "                print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc)+ \", Testing Accuracy= \" + \"{:.5f}\".format(test_acc)\n",
    "            \n",
    "            if(step * batch_size >= training_iters):\n",
    "                flag = True\n",
    "                break;\n",
    "            step += 1\n",
    "        if(flag):\n",
    "            break;\n",
    "    print \"Optimization Finished!\"\n",
    "    print \"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: t_example, y: t_label, keep_prob: 1.})\n",
    "    \n",
    "    test_example, test_label = data_process(test)\n",
    "    result_note = sess.run(pred, feed_dict={x:test_example,keep_prob: 1.})\n",
    "    print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write logits values into file\n",
    "np.savetxt(\"../DeepCNN_train0_1.csv\",result_note,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output file\n",
    "result = np.argmax(result_note,axis=1).reshape((test.shape[0],1))\n",
    "output = np.zeros((test.shape[0],2))\n",
    "output[:,0:1] = test[:,0:1]\n",
    "output[:,1:] = result\n",
    "np.savetxt(\"../DeepCNN0.csv\",output,fmt='%d',delimiter=\",\",header='Id,Character')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
